{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e24cd4",
   "metadata": {},
   "source": [
    "# Olympics & Countries ETL Pipeline\n",
    "\n",
    "This notebook reads the CSV files from `datasets/olympics` and `datasets/countries`, normalizes columns, creates an artificial `country_id` to align records across datasets, upserts results to Parquet files under `outputs/`, and writes a denormalized artifact suitable for programmatic queries.\n",
    "\n",
    "Outputs: `outputs/countries.parquet`, `outputs/olympics.parquet`, `outputs/olympics_denormalized.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552c7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 2.3.3\n",
      "DATA_DIR /workspaces/data-engineer-test/datasets\n"
     ]
    }
   ],
   "source": [
    "# Imports and path setup\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('pandas', pd.__version__)\n",
    "\n",
    "# Paths (detect repository root by looking for a 'datasets' folder or README.md)\n",
    "def find_repo_root(marker_names=('datasets', 'README.md')):\n",
    "    p = Path.cwd()\n",
    "    while True:\n",
    "        if any((p / m).exists() for m in marker_names):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    # fallback to current working directory\n",
    "    return Path.cwd()\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "DATA_DIR = REPO_ROOT / 'datasets'\n",
    "OLY_DIR = DATA_DIR / 'olympics'\n",
    "COUNTRIES_CSV = DATA_DIR / 'countries' / 'countries of the world.csv'\n",
    "OUT_DIR = REPO_ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('DATA_DIR', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6888f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def country_id_from_name(name: str) -> str:\n",
    "    \"\"\"Create a stable artificial key from a country name.\n",
    "    Uses a normalized lowercase form and MD5 to keep IDs compact and deterministic.\n",
    "    Handles NaN gracefully.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        name = ''\n",
    "    key = re.sub(r'\\s+', ' ', str(name)).strip().lower()\n",
    "    return hashlib.md5(key.encode('utf-8')).hexdigest()\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize column names: strip, lowercase, replace spaces with underscores.\n",
    "    \"\"\"\n",
    "    df = df.rename(columns=lambda c: re.sub(r'\\s+', '_', str(c).strip().lower()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eded454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and normalize all olympics CSV files into a single DataFrame\n",
    "def read_olympics_files() -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for f in sorted(OLY_DIR.glob('*.csv')):\n",
    "        # try to extract a year from filename (e.g., Tokyo 2020 ...)\n",
    "        m = re.search(r'(\\d{4})', f.name)\n",
    "        year = int(m.group(1)) if m else None\n",
    "        # attempt to read with a forgiving configuration\n",
    "        try:\n",
    "            # prefer the default engine; avoid engine='python' together with low_memory\n",
    "            df = pd.read_csv(f, low_memory=False, encoding='utf-8')\n",
    "        except Exception:\n",
    "            df = pd.read_csv(f, low_memory=False, encoding='latin1')\n",
    "        df = normalize_columns(df)\n",
    "        df['source_file'] = f.name\n",
    "        df['year'] = year\n",
    "        # unify country-like column names\n",
    "        for c in ('nation', 'country', 'team'):\n",
    "            if c in df.columns:\n",
    "                df = df.rename(columns={c: 'country'})\n",
    "                break\n",
    "        if 'country' not in df.columns:\n",
    "            df['country'] = None\n",
    "        # coerce numeric medal columns when present\n",
    "        for medal in ('gold', 'silver', 'bronze', 'total', 'rank'):\n",
    "            if medal in df.columns:\n",
    "                df[medal] = pd.to_numeric(df[medal].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce')\n",
    "        # create artificial country id\n",
    "        df['country_id'] = df['country'].astype(str).apply(country_id_from_name)\n",
    "        frames.append(df)\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "# Build olympics table (DataFrame)\n",
    "def build_olympics_table() -> pd.DataFrame:\n",
    "    df = read_olympics_files()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8cfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and normalize the countries CSV into a DataFrame\n",
    "def build_countries_table() -> pd.DataFrame:\n",
    "    # robust read (some versions of this CSV use commas and quoted fields)\n",
    "    try:\n",
    "        # avoid specifying engine='python' when using low_memory\n",
    "        df = pd.read_csv(COUNTRIES_CSV, low_memory=False, encoding='utf-8')\n",
    "    except Exception:\n",
    "        df = pd.read_csv(COUNTRIES_CSV, low_memory=False, encoding='latin1')\n",
    "    df = normalize_columns(df)\n",
    "    # find a sensible name column\n",
    "    for c in ('country', 'name', 'country_name'):\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c: 'country'})\n",
    "            break\n",
    "    if 'country' not in df.columns:\n",
    "        df['country'] = None\n",
    "    df['country_id'] = df['country'].astype(str).apply(country_id_from_name)\n",
    "    # attempt to coerce common numeric-like columns\n",
    "    numeric_like = []\n",
    "    for col in df.columns:\n",
    "        if re.search(r'(population|area|density|gdp|percapita|index|%|rate)', col):\n",
    "            numeric_like.append(col)\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b5b09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert helper for parquet files\n",
    "def upsert_parquet(df: pd.DataFrame, out_path: Path, key_cols):\n",
    "    \"\"\"Upsert rows from df into out_path using key_cols as the natural key.\n",
    "    If the parquet exists, read it, concatenate, and drop duplicates keeping the latest (by index of concatenation).\n",
    "    \"\"\"\n",
    "    if out_path.exists():\n",
    "        existing = pd.read_parquet(out_path)\n",
    "        combined = pd.concat([existing, df], ignore_index=True, sort=False)\n",
    "        # keep the last occurrence for each key (assuming later rows in df should win)\n",
    "        combined = combined.drop_duplicates(subset=key_cols, keep='last')\n",
    "    else:\n",
    "        combined = df.copy()\n",
    "    combined.to_parquet(out_path, index=False)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b340ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading countries CSV...\n",
      "Upserting countries -> /workspaces/data-engineer-test/outputs/countries.parquet\n",
      "Pipeline failed with exception: 'str' object has no attribute 'exists'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Execute pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m artifacts = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mWrote empty countries parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     countries_written = \u001b[43mupsert_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountries_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcountry_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mwrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(countries_written)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m country rows\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mReading olympics CSVs...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mupsert_parquet\u001b[39m\u001b[34m(df, out_path, key_cols)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupsert_parquet\u001b[39m(df: pd.DataFrame, out_path: Path, key_cols):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Upsert rows from df into out_path using key_cols as the natural key.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    If the parquet exists, read it, concatenate, and drop duplicates keeping the latest (by index of concatenation).\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mout_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m():\n\u001b[32m      7\u001b[39m         existing = pd.read_parquet(out_path)\n\u001b[32m      8\u001b[39m         combined = pd.concat([existing, df], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m, sort=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "# Run the full pipeline and write artifacts\n",
    "def run_pipeline():\n",
    "    try:\n",
    "        print('Reading countries CSV...')\n",
    "        countries = build_countries_table()\n",
    "        countries_path = \"/workspaces/data-engineer-test/outputs/countries.parquet\"\n",
    "        print(f'Upserting countries -> {countries_path}')\n",
    "        if countries is None or (hasattr(countries, 'empty') and countries.empty):\n",
    "            # If no new countries data, prefer existing parquet if present\n",
    "            if countries_path.exists():\n",
    "                countries_written = pd.read_parquet(countries_path)\n",
    "                print(f'No new countries found; using existing file with {len(countries_written)} rows')\n",
    "            else:\n",
    "                # write an empty parquet with expected structure\n",
    "                countries_written = countries.copy() if countries is not None else pd.DataFrame(columns=['country','country_id'])\n",
    "                countries_written.to_parquet(countries_path, index=False)\n",
    "                print('Wrote empty countries parquet')\n",
    "        else:\n",
    "            countries_written = upsert_parquet(countries, countries_path, key_cols=['country_id'])\n",
    "            print(f'wrote {len(countries_written)} country rows')\n",
    "\n",
    "        print('Reading olympics CSVs...')\n",
    "        olympics = build_olympics_table()\n",
    "        olympics_path = OUT_DIR / 'olympics.parquet'\n",
    "        print(f'Upserting olympics -> {olympics_path}')\n",
    "        if olympics is None or (hasattr(olympics, 'empty') and olympics.empty):\n",
    "            if olympics_path.exists():\n",
    "                olympics_written = pd.read_parquet(olympics_path)\n",
    "                print(f'No new olympics data; using existing file with {len(olympics_written)} rows')\n",
    "            else:\n",
    "                olympics_written = olympics.copy() if olympics is not None else pd.DataFrame(columns=['country','country_id','year'])\n",
    "                olympics_written.to_parquet(olympics_path, index=False)\n",
    "                print('Wrote empty olympics parquet')\n",
    "        else:\n",
    "            olympics_written = upsert_parquet(olympics, olympics_path, key_cols=['country_id', 'year'])\n",
    "            print(f'wrote {len(olympics_written)} olympics rows')\n",
    "\n",
    "        # Denormalize (left join olympics -> countries using country_id)\n",
    "        print('Creating denormalized artifact...')\n",
    "        merged = olympics_written.merge(countries_written, on='country_id', how='left', suffixes=('_olymp', '_country'))\n",
    "        merged_path = OUT_DIR / 'olympics_denormalized.parquet'\n",
    "        merged.to_parquet(merged_path, index=False)\n",
    "        print(f'wrote denormalized -> {merged_path} ({len(merged)} rows)')\n",
    "        return {'countries': countries_written, 'olympics': olympics_written, 'merged': merged}\n",
    "    except Exception as e:\n",
    "        print('Pipeline failed with exception:', e)\n",
    "        raise\n",
    "\n",
    "# Execute pipeline\n",
    "artifacts = run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks and example queries\n",
    "countries_df = artifacts['countries']\n",
    "olympics_df = artifacts['olympics']\n",
    "merged_df = artifacts['merged']\n",
    "\n",
    "print('Countries sample:')\n",
    "display(countries_df.head(5))\n",
    "\n",
    "print('Olympics sample:')\n",
    "display(olympics_df.head(5))\n",
    "\n",
    "print('Denormalized sample:')\n",
    "display(merged_df.head(5))\n",
    "\n",
    "# Example: Top 10 countries by total medals (if 'total' exists)\n",
    "if 'total' in olympics_df.columns:\n",
    "    top = olympics_df.groupby('country_id', dropna=False)['total'].sum().reset_index().sort_values('total', ascending=False).head(10)\n",
    "    # join back to get country name where available\n",
    "    top = top.merge(countries_df[['country_id','country']], on='country_id', how='left')\n",
    "    print('Top 10 by total medals:')\n",
    "    display(top)\n",
    "else:\n",
    "    print('No')\n",
    " medal column present in olympics dataset to aggregate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae56a8",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "- The notebook creates an artificial `country_id` using MD5(country_name_normalized). This keeps a deterministic mapping but may not resolve all name variants (e.g., 'USA' vs 'United States'). For improved matching, consider fuzzy matching (Python's `fuzzywuzzy` or `rapidfuzz`) or a canonical mapping table.\n",
    "- Parquet files are written with the default engine (pyarrow). Install `pyarrow` if not already present.\n",
    "- To run this notebook: install dependencies from `requirements.txt`, then open in Jupyter or run cells in your environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
